---
name: code-validation
description: AI-generated code validation checklist, quality audit dimensions, risk profile assessment, and fitness function patterns for evaluating codebase health
---
<!-- Based on Andrea Laforgia's claude-code-agents: https://github.com/andlaf-ak/claude-code-agents -->

# Code Validation

## When to Apply

Apply this skill when:
- The codebase was partially or fully generated by AI coding agents (Cursor, Copilot, Claude Code, Devin, etc.)
- The user explicitly requests a code quality audit
- Analysis reveals patterns consistent with AI generation (high file count with uniform style, formulaic naming, high test count with low assertion density)

## AI Code Risk Profile

Evidence-based risk factors (Veracode 2025, AIDev 2025):
- 45% of AI-generated code fails security tests across 100+ LLMs
- 43% of AI patches fix primary issue but introduce failures under adverse conditions
- Remediation takes 3x longer because teams must first understand code purpose
- "Happy path" bias: AI code passes standard tests but fails edge cases

## Validation Dimensions

### 1. Architectural Coherence

Check whether the codebase has consistent, intentional structure:

| Signal | Healthy | Concern |
|--------|---------|---------|
| Module boundaries | Clear responsibilities, minimal coupling | Modules with mixed concerns, high fan-out |
| Layer discipline | Dependencies flow one direction | Skip-layer dependencies, circular references |
| Naming consistency | Uniform conventions across codebase | Mixed naming styles (camelCase + snake_case in same layer) |
| Pattern consistency | Same problem solved the same way | Multiple approaches to the same concern (3 different error handling patterns) |
| Abstraction depth | Appropriate for complexity | Over-abstraction (5 layers for a CRUD operation) or under-abstraction (god classes) |

### 2. Decision Rationale Gaps

AI agents make design choices without documenting rationale. Flag:

- Technology choices with no ADR or README explanation
- Architectural patterns that don't match the project's scale (microservices for a 10-file project)
- Framework choices that conflict with team expertise signals (e.g., choosing an obscure framework when the git history shows expertise in a mainstream one)
- Configuration choices with default values that may not suit production

### 3. Test Effectiveness

Beyond coverage percentage, assess whether tests actually verify behavior:

| Metric | Healthy Range | Red Flag |
|--------|--------------|----------|
| Assertion density | 2-5 assertions per test | <1.5 assertions per test (passive tests) |
| Mock ratio | <2 mocks per test on average | >3 mocks per test (testing mocks, not code) |
| Test-to-code ratio | 0.8-1.5 test files per prod file | <0.5 (undertested) or >2.0 (possible test bloat) |
| Assertion-free tests | 0% | Any test with zero assertions |
| Circular verification | None | Tests that duplicate production logic in assertions |

### 4. Security Patterns

Scan for common AI-generated security issues:

- **Input validation**: Are API endpoints validating input? Or trusting all incoming data?
- **Authentication/Authorization**: Are auth checks present on protected routes? Or only on some?
- **Secret management**: Are secrets hardcoded? In environment variables? In a vault?
- **SQL injection**: Raw string interpolation in queries?
- **Error exposure**: Stack traces or internal details in error responses?
- **Dependency vulnerabilities**: Known CVEs in dependencies?

### 5. Consistency Audit

AI agents working over long sessions can drift in style. Check:

- Error handling: Is it consistent across all modules? (Some catch, some throw, some ignore?)
- Logging: Same logging library and pattern everywhere? Or mixed approaches?
- Configuration: One approach to env vars? Or some hardcoded, some from config files?
- Naming: Consistent entity naming? Or `User` in one place, `UserEntity` in another, `UserModel` in a third?

### 6. Over-Engineering Signals

AI agents tend to over-engineer. Flag:

- Abstractions with single implementations (interface + single class for no reason)
- Factory patterns for objects created once
- Strategy patterns with one strategy
- Deep class hierarchies (>3 levels) for simple concepts
- Wrapper classes that add no behavior
- Configuration systems for values that never change

## Validation Checklist

Run through these checks and report findings per category:

```
[ ] ARCHITECTURE
    [ ] Module boundaries are clear and consistent
    [ ] Dependencies flow in one direction (no cycles)
    [ ] No god classes (>500 LOC or >15 methods)
    [ ] No god functions (>50 LOC or >5 nesting levels)
    [ ] Consistent patterns across the codebase

[ ] DECISIONS
    [ ] Major technology choices have documented rationale (or reasonable inferred rationale)
    [ ] Architecture style matches project scale
    [ ] Framework choices align with apparent team expertise

[ ] TESTING
    [ ] Assertion density >= 2.0 per test
    [ ] No assertion-free tests
    [ ] Mock ratio < 3.0 per test
    [ ] No circular verification patterns
    [ ] Test-to-code ratio between 0.5 and 2.0

[ ] SECURITY
    [ ] Input validation on API endpoints
    [ ] Auth checks on protected routes
    [ ] No hardcoded secrets
    [ ] No raw SQL string interpolation
    [ ] No stack traces in error responses

[ ] CONSISTENCY
    [ ] Uniform error handling approach
    [ ] Consistent logging patterns
    [ ] Single configuration approach
    [ ] Consistent naming conventions

[ ] ENGINEERING
    [ ] No unnecessary abstractions (single-implementation interfaces)
    [ ] No pattern overkill (factory for single creation, strategy for single strategy)
    [ ] No deep hierarchies for simple concepts
    [ ] Appropriate complexity for the problem domain
```

## Presenting Findings

Weave validation findings into the walkthrough slides, not as a separate audit section. For each walkthrough section:

- **Architecture slides**: Note conformance or violations inline ("This layered structure is clean, with one exception: the OrderService directly accesses the database client, bypassing the repository layer")
- **Code walkthrough slides**: Flag quality concerns in context ("This module has high hotspot score AND low assertion density â€” changes here are risky")
- **Test slides**: Present both metrics and their implications
- **A summary "Health" slide** in Section 6 (Risks) should aggregate findings into a clear risk profile

## Health Score

Compute a simple health score for the risk summary slide:

```
For each dimension (Architecture, Testing, Security, Consistency, Engineering):
  - All checks pass: GREEN (2 points)
  - Minor issues only: YELLOW (1 point)
  - Any major issue: RED (0 points)

Total: /10
  8-10: Healthy codebase
  5-7:  Attention needed in specific areas
  0-4:  Significant quality concerns
```

Report the score with the specific failing checks, not just the number.